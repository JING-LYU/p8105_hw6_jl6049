---
title: "p8105_hw6_jl6049"
author: "LYU JING"
date: "11/27/2021"
output: github_document
---

```{r message=FALSE}

library(ALSM)
library(modelr)
library(tidyverse)
```

# Problem 1

**Load and clean the data for regression analysis**

```{r}
birthweight = read.csv("birthweight.csv")
```

```{r}

skimr::skim(birthweight)

```

The dataset has No missing. But `pnumlbw` and `pnumsga` are all zero, so cancel those two columns. Then, Convert several numeric variables to factor.
```{r}
birthweight_clean =birthweight %>% 
  select(-pnumlbw,-pnumsga) %>% 
  mutate(babysex = factor(babysex),
         frace = factor(frace),
         malform = factor(malform),
         mrace = factor(mrace))
```



**Propose a regression model for birthweight.**

Use functions below to find out proper variables to fit our model.

```{r eval = FALSE}
variables = birthweight %>% select(-bwt,-pnumlbw,-pnumsga)

cor(variables)

variables = variables %>% select(-frace,-ppbmi,-ppwt,-ppbmi) 


BestSub(variables,birthweight$bwt) # best subset

variables = variables %>% select(-malform) 

```
Use `cor()` to find out correlation between variables:

- `ppwt`,`ppbmi` ,`delwt` and `wtgain` have relation, so I only keep `wtgain` and`delwt`.
- `mrace` and `frace` are correlated to each other, so only keep `mrace`

Use `BestSub()`:

- `malform` seems not influence the model a lot so cancel it.


Based on the understanding, I guess `fincme` and `momage`and `menarche` are not relevant to response. So I try to figure this out by evaluating p values of those variables in a lm model.
```{r}
fit = lm(bwt ~ babysex + bhead + blength +delwt + fincome + gaweeks + menarche + mheight + momage + mrace  +parity +smoken+  wtgain , data = birthweight_clean)
summary(fit)
```

I find out `fincome` and `momage`and `menarche` is not significant then cancel it.


Fit the final model:
```{r}
fit_final = lm(bwt ~ babysex + bhead + blength +delwt + gaweeks + mheight + mrace  +parity +smoken+  wtgain , data = birthweight_clean)
summary(fit_final)
```

**Show a plot of model residuals against fitted values**
```{r}
birthweight_clean %>% 
  add_predictions(fit_final) %>% 
  add_residuals(fit_final) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point(alpha = 0.1) +
  labs(
    title = "plot of model residuals against fitted values", 
    x = "Predict values",
    y = "Residuals"
  ) 
```


**Compare my model to two other models**

- One using length at birth and gestational age as predictors (main effects only)
- One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}
fit_main = lm(bwt ~ blength + gaweeks, data = birthweight_clean)

fit_interaction = lm(bwt ~ babysex + bhead + blength + babysex*bhead+bhead*blength+babysex*blength+babysex*bhead*blength , data = birthweight_clean)

```

Make this comparison in terms of the cross-validated prediction error; 

```{r}
cv_df = 
  crossv_mc(birthweight_clean, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) %>% 
  mutate(
    fit_final  = map(train, ~lm(bwt ~ babysex + bhead + blength +delwt + gaweeks + mheight + mrace  +parity +smoken+  wtgain , data = .x)),
    fit_main  = map(train, ~ lm(bwt ~ blength + gaweeks, data = .x)),
    fit_interaction  = map(train, ~lm(bwt ~ babysex + bhead + blength + babysex*bhead+bhead*blength+babysex*blength+babysex*bhead*blength , data = .x))) %>% 
  mutate(
    rmse_final = map2_dbl(fit_final, test, ~rmse(model = .x, data = .y)),
    rmse_main = map2_dbl(fit_main, test, ~rmse(model = .x, data = .y)),
    rmse_nteraction = map2_dbl(fit_interaction, test, ~rmse(model = .x, data = .y)))

```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin(aes(fill = model)) +
  labs(
    title = "comparison of the cross-validated prediction error", 
    x = "models",
    y = "rmse"
  ) 
```

# problem 2

```{r message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities 

```{r message=FALSE}
set.seed(1)
estimates =
  weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results1 = map(models, broom::glance),
    results2 = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results1) %>% 
  select(.id,r.squared,results2) %>% 
  unnest(results2) %>% 
  group_by(.id) %>% 
  summarize(
    r.squared = r.squared,
    logb0b1 = log(prod(estimate))) %>% 
  unique() %>% 
  ungroup()

```

Plot the distribution of your estimates, and describe these in words. 

```{r}
estimates %>% 
  select(r.squared) %>% 
  ggplot(aes(x = r.squared)) + geom_density() +
  labs(
    title = "distribution of R-square", 
    x = "R^2"
  ) 
```

The mean of R square distribution is around 0.91, means that the R squared estimates mostly lays around 0.91. This distribution has a heavy tail extending to low values, which could means that some bootstrap samples have quite low R-square. Thus, it indicates that there are some outliers that their independent variables is not explaining much in the variation of my dependent variable

```{r}
estimates %>% 
  select(logb0b1) %>% 
  ggplot(aes(x = logb0b1)) + geom_density() +
  labs(
    title = "distribution of log(b0*b1)",
    x = "log(b0*b1)")
```

This distribution of $\log(\hat{\beta}_0 * \hat{\beta}_1)$ is generally normal with mean around 2.02, means the estimates mostly lays around 2.02. But the distribution have a little "shoulder" near 2.025. 


```{r}
ci_r =
estimates %>%
  summarise(qlow_r = quantile(r.squared,probs = c(0.025)),
            qhigh_r = quantile(r.squared,probs = c(0.975)))

ci_r %>% knitr::kable()

ci_b =
estimates %>%
  summarise(qlow_b = quantile(logb0b1,probs = c(0.025)),
            qhigh_b = quantile(logb0b1,probs = c(0.975)))

ci_b %>% knitr::kable()

```


